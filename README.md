# Evaluation of Open Source LLMs for OntoGenix
This repository describes and reports the results obtained by applying the OntoGenix pipeline with open source large language models (openLLMs).

## openLLMs used
* [Llama-2](https://huggingface.co/meta-llama/Llama-2-7b): 7B and 70B
* [Llama-3](https://github.com/meta-llama/llama3): 8B and 70B
* [Mixtral](https://huggingface.co/docs/transformers/en/model_doc/mixtral)
* [Mistral](https://mistral.ai): 7B

## Experiments 

The experiments consisted in generating ontologies for the Kaggle datasets used in the [original experiments with OpenAI GPT](https://github.com/tecnomod-um/OntoGenixEvaluation).

Therefore, each experiment with a particular configuration of an OpenLLM consisted in generating an ontology from one of those datasets. A page will describe each type of experiment, and the results of each openLLM will be found in a specific folder. 

These are the types of experiments performed:

* [Base openLLM](./base-llm/README.md): Use of the OntoGenix prompts with the base distribution of the openLLM. These experiments are performed only with openLLMs
 
* [Fine-tuning](./fine-tuning/README.md): Use of the OntoGenix prompts with fine-tuned LLMs. These experiments are performed with both openLLMs and openAI GPT.

* [RAG](./rag/README.md): Use of the OntoGenix prompts with base LLMs with Retrieved Augmented Generation (RAG). These experiments are performed with both openLLMs and openAI GPT.


## Summary of results
The next tables summarize the perceived quality of the ontologies generated in these experiments, and including the ontologies generated by the corresponding GPT models, when compared to the gold standard ontology in each dataset.

First, we indicate which approach most closely replicated the gold standard human-generated ontology. This comparison is done for each of the datasets. 

| Dataset           | Base GPT-4o | Fine-tuning GPT-4o | RAG GPT-4o |
|-------------------|-------------|--------------------|------------|
| Amazon            | X           |                    | X          |
| Airlines          |             |                    | X          |
| BigBasket         |             |                    | X          |
| Brazilian         |             | X                  |            |
| CustomerFeedback  | X           |                    |            |
| eCommerce         |             | X                  | X          |


Next, we include a cualitative valoration of the proximity between the ontology generated by the LLM and the gold standard ontology.
Being the desing between them:
<br>
1 - Distanct <br>
2 - Proximal <br>
3 - Close <br>
4 - Equal <br>

| Dataset           |  Base GPT-4o | Fine-tuning GPT-4o | RAG GPT-4o |
|-------------------|--------------|--------------------|------------|
| Amazon            | 4            | 3                  | 4          |
| Airlines          | 1            | 2                  | 3          |
| BigBasket         | 2            | 1                  | 3          |
| Brazilian         | 2            | 3                  | 2          |
| CustomerFeedback  | 1            | 1                  | 1          |
| eCommerce         | 1            | 3                  | 3          |
| **Mean**          | **1.83**     | **2.17**           | **2.67**   |

From these results we highlight two main aspects:
- First, the models with fine-tuning and RAG applied in GPT-4o showed better results than the base model 4o in most cases. Consequently, these approaches allow improving the efficiency of the base model against specific tasks, such as the generation of ontological schemas from real data files.
- Secondly, RAG GTP-4o is the approach that shows the best results. However, it is not the optimal option in all cases, i.e., the approach that obtains the closest ontologies to the human gold standards may differ according to the dataset to be modelled.
